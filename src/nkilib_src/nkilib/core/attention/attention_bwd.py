# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Flash Attention Backward Pass Kernel for NKI.

This module implements the backward pass of Flash Attention algorithm optimized
for NeuronCore. It supports:
- Standard multi-head attention
- Grouped-query attention (GQA)
- Multi-query attention (MQA)
- Causal masking
- Sliding window attention

The implementation uses tiled computation to handle long sequences efficiently.
"""

from dataclasses import dataclass
from typing import Any, List, Optional, Tuple

import nki
import nki.isa as nisa
import nki.language as nl

from ..utils.kernel_assert import kernel_assert
from ..utils.kernel_helpers import div_ceil

_FLOAT32_MIN: float = -3.4028235e38  # Value for masked attention positions


@nki.jit
def attention_bwd(
    q_ref: nl.ndarray,
    k_ref: nl.ndarray,
    v_ref: nl.ndarray,
    o_ref: nl.ndarray,
    dy_ref: nl.ndarray,
    lse_ref: nl.ndarray,
    sinks_ref: Optional[nl.ndarray] = None,
    use_causal_mask: bool = False,
    mixed_precision: bool = False,
    softmax_scale: Optional[float] = None,
    sliding_window: Optional[int] = None,
) -> Tuple[nl.ndarray, nl.ndarray, nl.ndarray]:
    """
    Flash Attention backward pass kernel.

    Computes gradients dQ, dK, dV using the memory-efficient Flash Attention
    algorithm, which recomputes attention scores during backprop to avoid
    materializing the full attention matrix.

    Dimensions:
        bs: Batch size
        nheads: Number of query attention heads
        nheads_kv: Number of key/value attention heads
        head_dim (d_head): Head dimension size
        seq_len_q (seqlen_q): Query sequence length
        seq_len_k (seqlen_k): Key/value sequence length

    Args:
        q_ref (nl.ndarray): [bs, nheads, head_dim, seq_len_q], Query tensor in HBM
        k_ref (nl.ndarray): [bs, nheads_kv, head_dim, seq_len_k], Key tensor in HBM
        v_ref (nl.ndarray): [bs, nheads_kv, head_dim, seq_len_k], Value tensor in HBM
        o_ref (nl.ndarray): [bs, nheads, head_dim, seq_len_q], Forward pass output in HBM
        dy_ref (nl.ndarray): [bs, nheads, head_dim, seq_len_q], Upstream gradient in HBM
        lse_ref (nl.ndarray): [bs, nheads, tile_size, seq_len_q // tile_size],
            Log-sum-exp from forward pass in HBM
        sinks_ref (Optional[nl.ndarray]): [bs, nheads] or [bs, nheads, num_sinks], Attention sinks tensor in HBM.
        use_causal_mask (bool): If True, apply causal masking
        mixed_precision (bool): If True, use float32 for intermediate computations
        softmax_scale (Optional[float]): Scaling factor for attention scores. Defaults to 1/sqrt(head_dim).
        sliding_window (Optional[int]): Sliding window size for local attention. None to disable.

    Returns:
        dq (nl.ndarray): [bs, nheads, head_dim, seq_len_q], Gradient with respect to Q in HBM
        dk (nl.ndarray): [bs, nheads_kv, head_dim, seq_len_k], Gradient with respect to K in HBM
        dv (nl.ndarray): [bs, nheads_kv, head_dim, seq_len_k], Gradient with respect to V in HBM
        dsinks (Optional[nl.ndarray]): [bs, nheads] or [bs, nheads, num_sinks], Gradient with respect to sinks in HBM

    Notes:
        - Supports standard multi-head attention, GQA, and MQA
        - Causal masking requires seq_len_q == seq_len_k
        - Sliding window is only supported with causal masking
        - All input tensors must have the same dtype

    Pseudocode:
        # Step 1: Compute D = rowsum(dO ◦ O) (pointwise multiply)
        D = rowsum(dy_ref * o_ref)

        # Step 2: Recompute attention scores
        # 2.1: Compute Q^T @ K
        scores = matmul(Q, K.T)
        # 2.2: Scale the QK score
        scores = scores * softmax_scale
        # 2.3: Apply causal mask
        if use_causal_mask:
            scores = apply_mask(scores)
        # 2.4: Apply softmax
        attn_weights = softmax(scores)

        # Step 3: Compute gradients through value projection
        # dL/dV = attn_weights^T @ dY
        dV = matmul(attn_weights.T, dy_ref)

        # Step 4: Compute gradients through softmax
        # dL/d(scores) = attn_weights * (dL/d(attn_weights) - D)
        d_attn = attn_weights * (matmul(dy_ref, V.T) - D)

        # Step 5: Compute gradients through Q^T @ K
        # 5.1: Compute dQ = d_attn @ K * softmax_scale
        dQ = matmul(d_attn, K) * softmax_scale
        # 5.2: Compute dK = Q^T @ d_attn (Q already scaled)
        dK = matmul(Q.T, d_attn)

        return dQ, dK, dV
    """
    bs, nheads, d_head, seqlen_q = q_ref.shape
    _, nheads_kv, _, seqlen_k = k_ref.shape
    sliding_window = sliding_window or 0

    validate_inputs(q_ref, k_ref, v_ref, o_ref, dy_ref, lse_ref, sinks_ref, use_causal_mask, sliding_window)

    # Softmax scaling factor, multiplied onto Q
    softmax_scale = softmax_scale or 1.0 / float(d_head**0.5)
    sliding_window = min(sliding_window, seqlen_k)

    out_dq_ref = nl.ndarray((bs, nheads, d_head, seqlen_q), dtype=q_ref.dtype, buffer=nl.shared_hbm)
    out_dk_ref = nl.ndarray((bs, nheads_kv, d_head, seqlen_k), dtype=k_ref.dtype, buffer=nl.shared_hbm)
    out_dv_ref = nl.ndarray((bs, nheads_kv, d_head, seqlen_k), dtype=v_ref.dtype, buffer=nl.shared_hbm)
    out_dsinks_ref = None

    if sinks_ref is not None:
        out_dsinks_ref = nl.ndarray(tuple(sinks_ref.shape), dtype=sinks_ref.dtype, buffer=nl.shared_hbm)

    flash_attn_bwd(
        out_dq_ref,
        out_dk_ref,
        out_dv_ref,
        out_dsinks_ref,
        q_ref,
        k_ref,
        v_ref,
        o_ref,
        dy_ref,
        lse_ref,
        sinks_ref,
        use_causal_mask,
        mixed_precision,
        softmax_scale,
        sliding_window,
    )

    if sinks_ref is not None:
        return out_dq_ref, out_dk_ref, out_dv_ref, out_dsinks_ref
    return out_dq_ref, out_dk_ref, out_dv_ref


def validate_inputs(
    q_ref: nl.ndarray,
    k_ref: nl.ndarray,
    v_ref: nl.ndarray,
    o_ref: nl.ndarray,
    dy_ref: nl.ndarray,
    lse_ref: nl.ndarray,
    sinks_ref: Optional[nl.ndarray],
    use_causal_mask: bool,
    sliding_window: int,
) -> None:
    """
    Validate input tensor shapes for attention backward pass.

    Args:
        q_ref (nl.ndarray): Query tensor, shape (bs, nheads, d_head, seqlen_q).
        k_ref (nl.ndarray): Key tensor, shape (bs, nheads_kv, d_head, seqlen_k).
        v_ref (nl.ndarray): Value tensor, shape (bs, nheads_kv, d_head, seqlen_k).
        o_ref (nl.ndarray): Forward output tensor, shape (bs, nheads, d_head, seqlen_q).
        dy_ref (nl.ndarray): Gradient tensor, shape (bs, nheads, d_head, seqlen_q).
        lse_ref (nl.ndarray): Log-sum-exp tensor, shape (bs, nheads, pmax, seqlen_q // pmax).
        sinks_ref (Optional[nl.ndarray]): Attention sinks, shape (bs, nheads) or (bs, nheads, num_sinks).
        use_causal_mask (bool): Whether causal masking is enabled.
        sliding_window (int): Sliding window size for local attention.

    Returns:
        None

    Notes:
        - Validates GQA/MQA head compatibility (nheads divisible by nheads_kv)
        - Ensures all tensors have consistent batch size and head dimensions
        - Verifies causal masking constraint (seqlen_q == seqlen_k)
        - Checks dtype consistency across all input tensors
    """
    bs, nheads, d_head, seqlen_q = q_ref.shape
    _, nheads_kv, _, seqlen_k = k_ref.shape
    kernel_assert(
        (nheads == nheads_kv) or (nheads % nheads_kv == 0),
        (f"Query heads {nheads} should be equal to or divisible by key/value heads ({nheads_kv})"),
    )
    kernel_assert(tuple(k_ref.shape) == (bs, nheads_kv, d_head, seqlen_k), f"Input K shape mismatch, got {k_ref.shape}")
    kernel_assert(tuple(v_ref.shape) == (bs, nheads_kv, d_head, seqlen_k), f"Input V shape mismatch, got {v_ref.shape}")
    kernel_assert(tuple(o_ref.shape) == (bs, nheads, d_head, seqlen_q), f"Input o shape mismatch, got {o_ref.shape}")
    kernel_assert(tuple(dy_ref.shape) == (bs, nheads, d_head, seqlen_q), f"Input dy shape mismatch, got {dy_ref.shape}")
    kernel_assert(
        tuple(lse_ref.shape)
        == (
            bs,
            nheads,
            nl.tile_size.pmax,
            seqlen_q // nl.tile_size.pmax,
        ),
        f"Input lse shape mismatch, got {lse_ref.shape}",
    )
    kernel_assert(
        not use_causal_mask or seqlen_q == seqlen_k,
        (f"Causal attention assumes same sequence length for query and key, got {seqlen_q} and {seqlen_k}"),
    )
    kernel_assert(
        q_ref.dtype == k_ref.dtype == v_ref.dtype == o_ref.dtype == dy_ref.dtype,
        f"Got multiple input dtypes {q_ref.dtype} {k_ref.dtype} {v_ref.dtype} {o_ref.dtype} {dy_ref.dtype}",
    )
    if sinks_ref is not None:
        sinks_shape = (bs, nheads, sinks_ref.shape[-1]) if len(sinks_ref.shape) == 3 else (bs, nheads)
        kernel_assert(
            tuple(sinks_ref.shape) == sinks_shape,
            f"Attention sinks may need to be replicated along batch dim, required shape {sinks_shape}, got {sinks_ref.shape}",
        )
    kernel_assert(sliding_window <= 0 or use_causal_mask, "Sliding window is supported for causal attention only")


@dataclass
class AttentionBwdConfig(nl.NKIObject):
    """
    Configuration container for Flash Attention backward pass.

    Holds all configuration parameters, tensor shapes, tile sizes, and
    precomputed offsets needed throughout the backward pass computation.

    Attributes:
        kernel_dtype: Data type for kernel computations.
        mixed_dtype: Data type for accumulations (float32 if mixed_precision).
        bs: Batch size.
        nheads: Number of query attention heads.
        nheads_kv: Number of key/value attention heads.
        nheads_per_kv_head: Query heads per key/value head (nheads // nheads_kv).
        d_head: Head dimension size.
        seqlen_q: Query sequence length.
        seqlen_k: Key/value sequence length.
        q_seq_tile_size: Tile size for query sequence dimension.
        k_seq_tile_size: Tile size for key sequence dimension.
        k_seq_tile_size_backward: Transposed tile size for key sequence dimension.
        d_head_tile_size: Tile size for head dimension.
        q_tile_group_size: Number of Q tiles processed together.
        d_head_n_tiles: Number of tiles covering head dimension.
        q_seq_n_tiles: Number of tiles covering query sequence.
        k_seq_n_tiles: Number of tiles covering key sequence.
        k_seq_fwd_bwd_multiplier: Number of transposed tiles in one K tile.
        offset_q_head: Stride to next query head.
        offset_q_bs: Stride to next batch in query tensor.
        offset_k_head: Stride to next key/value head.
        offset_k_bs: Stride to next batch in key/value tensor.
        offset_lse_head: Stride to next head in LSE tensor.
        offset_lse_bs: Stride to next batch in LSE tensor.
    """

    # Data types
    kernel_dtype: Any
    mixed_dtype: Any

    # Tensor shapes
    bs: int
    nheads: int
    nheads_kv: int
    nheads_per_kv_head: int
    d_head: int
    seqlen_q: int
    seqlen_k: int

    # Tile sizes
    q_seq_tile_size: int
    k_seq_tile_size: int
    k_seq_tile_size_backward: int
    d_head_tile_size: int
    q_tile_group_size: int

    # Computed tile counts
    d_head_n_tiles: int
    q_seq_n_tiles: int
    k_seq_n_tiles: int
    k_seq_fwd_bwd_multiplier: int

    # Tensor offsets
    offset_q_head: int
    offset_q_bs: int
    offset_k_head: int
    offset_k_bs: int
    offset_lse_head: int
    offset_lse_bs: int

    k_seq_section_len: int

    # Sinks
    num_sinks: int
    offset_sinks_bs: int
    offset_sinks_head: int


def setup_config(
    q_ref: nl.ndarray,
    k_ref: nl.ndarray,
    sinks_ref: Optional[nl.ndarray],
    mixed_precision: bool,
    q_seq_tile_size: int = nl.tile_size.pmax,
    k_seq_tile_size: int = nl.tile_size.psum_fmax,
    k_seq_tile_size_backward: int = nl.tile_size.pmax,
    d_head_tile_size: int = nl.tile_size.pmax,
    q_tile_group_size: int = 4,
    k_seq_section_len: int = 8192,
) -> AttentionBwdConfig:
    """
    Create configuration for flash attention backward pass.

    Extracts shapes from input tensors and computes all derived values
    needed for the backward pass.

    Args:
        q_ref: Query tensor, shape (bs, nheads, d_head, seqlen_q).
        k_ref: Key tensor, shape (bs, nheads_kv, d_head, seqlen_k).
        sinks_ref: Attention sinks, shape (bs, nheads) or (bs, nheads, num_sinks).
        mixed_precision: If True, use float32 for intermediate accumulations.
        q_seq_tile_size: Tile size for query sequence dimension.
        k_seq_tile_size: Tile size for key sequence dimension.
        k_seq_tile_size_backward: Transposed tile size for key sequence dimension.
        d_head_tile_size: Tile size for head dimension.
        q_tile_group_size: Number of Q tiles to process together.
        k_seq_section_len: Size for KV sequence sections to process at a time.

    Returns:
        AttentionBwdConfig with all computed configuration values.

    Raises:
        AssertionError: If sequence lengths are not divisible by tile sizes.
    """
    # Extract shapes
    bs, nheads, d_head, seqlen_q = q_ref.shape
    _, nheads_kv, _, seqlen_k = k_ref.shape

    # Data types
    kernel_dtype = q_ref.dtype
    mixed_dtype = nl.float32 if mixed_precision else kernel_dtype

    # Adjust tile sizes
    required_d_head_tiles = div_ceil(d_head, d_head_tile_size)
    d_head_tile_size = div_ceil(d_head, required_d_head_tiles)
    k_seq_tile_size = min(k_seq_tile_size, seqlen_k)

    # Max section length based on buffers allocated for k_loaded, v_loaded, dk and dv
    k_seq_section_len = k_seq_section_len // power_of_2(div_ceil(d_head, d_head_tile_size))

    kernel_assert(
        d_head % d_head_tile_size == 0,
        f"Can not create tiles of equal size for given head dimension {d_head}",
    )
    kernel_assert(
        seqlen_k % k_seq_tile_size == 0, f"Key sequence length should be multiple of {k_seq_tile_size}, got {seqlen_k}"
    )
    kernel_assert(
        k_seq_section_len % k_seq_tile_size == 0,
        f"Key section length should be multiple of {k_seq_tile_size}, got {k_seq_section_len}",
    )
    kernel_assert(
        k_seq_tile_size % k_seq_tile_size_backward == 0,
        f"Key sequence tile size should be multiple of {k_seq_tile_size_backward}, got {k_seq_tile_size}",
    )
    kernel_assert(
        seqlen_q % q_seq_tile_size == 0,
        f"Query sequence length should be multiple of {q_seq_tile_size}, got {seqlen_q}",
    )

    # Compute tile counts
    d_head_n_tiles = d_head // d_head_tile_size
    q_seq_n_tiles = seqlen_q // q_seq_tile_size
    k_seq_n_tiles = seqlen_k // k_seq_tile_size
    k_seq_fwd_bwd_multiplier = k_seq_tile_size // k_seq_tile_size_backward

    if sinks_ref is not None:
        num_sinks = sinks_ref.shape[-1] if len(sinks_ref.shape) == 3 else 1
        offset_sinks_head = num_sinks
        offset_sinks_bs = offset_sinks_head * nheads
    else:
        num_sinks, offset_sinks_bs, offset_sinks_head = 0, 0, 0

    return AttentionBwdConfig(
        # Data types
        kernel_dtype=kernel_dtype,
        mixed_dtype=mixed_dtype,
        # Tensor shapes
        bs=bs,
        nheads=nheads,
        nheads_kv=nheads_kv,
        nheads_per_kv_head=nheads // nheads_kv,
        d_head=d_head,
        seqlen_q=seqlen_q,
        seqlen_k=seqlen_k,
        # Tile sizes
        q_seq_tile_size=q_seq_tile_size,
        k_seq_tile_size=k_seq_tile_size,
        k_seq_tile_size_backward=k_seq_tile_size_backward,
        d_head_tile_size=d_head_tile_size,
        q_tile_group_size=q_tile_group_size,
        # Tile counts
        d_head_n_tiles=d_head_n_tiles,
        q_seq_n_tiles=q_seq_n_tiles,
        k_seq_n_tiles=k_seq_n_tiles,
        k_seq_fwd_bwd_multiplier=k_seq_fwd_bwd_multiplier,
        # Tensor offsets
        offset_q_head=d_head * seqlen_q,
        offset_q_bs=nheads * d_head * seqlen_q,
        offset_k_head=d_head * seqlen_k,
        offset_k_bs=nheads_kv * d_head * seqlen_k,
        offset_lse_head=q_seq_tile_size * q_seq_n_tiles,
        offset_lse_bs=nheads * q_seq_tile_size * q_seq_n_tiles,
        # Section length
        k_seq_section_len=k_seq_section_len,
        # Attention sinks
        num_sinks=num_sinks,
        offset_sinks_bs=offset_sinks_bs,
        offset_sinks_head=offset_sinks_head,
    )


def power_of_2(n: int) -> int:
    """
    Round up to nearest power of 2.

    Args:
        n (int): Input integer value.

    Returns:
        int: Smallest power of 2 greater than or equal to n.
    """
    p = 1
    while p < n:
        p *= 2
    return p


def ndarray(
    block_dim: Tuple[int, ...],
    tile_shape: Tuple[int, int],
    dtype: Any,
    value: Optional[float] = None,
    buffer: Any = nl.sbuf,
) -> List:
    """
    Allocate a nested list of NKI tiles.

    Args:
        block_dim (Tuple[int, ...]): Shape of the nested list structure, e.g., (2, 3) creates
            a 2-element list where each element is a 3-element list.
        tile_shape (Tuple[int, int]): Shape of each individual tile (rows, cols).
        dtype (Any): Data type for tiles.
        value (Optional[float]): If provided, initialize tiles to this value using memset.
        buffer (Any): NKI buffer type (default: nl.sbuf).

    Returns:
        List: Nested list of nl.ndarray tiles with structure matching block_dim.

    Notes:
        - Creates a flattened list of tiles then reshapes to nested structure
        - All tiles are allocated with the same shape and dtype

    Example:
        >>> tiles = ndarray((2, 3), (128, 512), nl.float32)
        >>> tiles[0][1]  # Access a 128x512 tile
    """
    # Total number of tiles, used to flatten and unflatten lists.
    num_tiles = 1
    for dim_size in block_dim:
        num_tiles *= dim_size

    # Allocate tiles in a flattened list.
    tiles = []
    for _ in range(num_tiles):
        tiles.append(nl.ndarray(tile_shape, dtype=dtype, buffer=buffer))
        if value is not None:
            nisa.memset(tiles[-1], value=value)

    # Reshape flattened list of tiles to nested nd-list of tiles.
    block_dim_len = len(block_dim)
    for dim_idx in range(block_dim_len):
        dim_size = block_dim[block_dim_len - 1 - dim_idx]

        tiles_reshaped = []
        num_tiles = num_tiles // dim_size
        for tile_idx in range(num_tiles):
            tiles_reshaped.append(tiles[tile_idx * dim_size : (tile_idx + 1) * dim_size])

        tiles = tiles_reshaped

    # Remove extra dimension (due to appending slice of lists earlier)
    return tiles[0]


def transpose_tiles(
    src_tensor: nl.ndarray,
    dst_tensor: nl.ndarray,
    src_tile_size: int,
    engine: Optional[Any] = None,
) -> None:
    """
    Transpose tiles from source to destination tensor.

    Performs nc_transpose: (P, src_tile_size) -> (src_tile_size, P) for each tile.

    Args:
        src_tensor (nl.ndarray): Source tensor, shape (P, src_tile_size * num_tiles).
        dst_tensor (nl.ndarray): Destination tensor, shape (src_tile_size, P * num_tiles).
        src_tile_size (int): Size of each tile in source's free dimension.
        engine (Optional[Any]): Optional engine for tensor_copy (e.g., nisa.scalar_engine).

    Returns:
        None
    """
    dst_tile_size = src_tensor.shape[0]
    num_tiles = src_tensor.shape[1] // src_tile_size
    out_row_size = src_tile_size

    step_size = min(nl.tile_size.psum_fmax // dst_tile_size, num_tiles)

    for start_idx in range(0, num_tiles, step_size):
        chunk_size = min(step_size, num_tiles - start_idx)
        end_idx = start_idx + chunk_size

        transposed_psum = nl.ndarray(
            (out_row_size, dst_tile_size * chunk_size),
            dtype=src_tensor.dtype,
            buffer=nl.psum,
        )

        for tile_idx in range(start_idx, end_idx):
            tile_idx_local = tile_idx - start_idx

            nisa.nc_transpose(
                transposed_psum[:, tile_idx_local * dst_tile_size : (tile_idx_local + 1) * dst_tile_size],
                src_tensor[:, tile_idx * src_tile_size : (tile_idx + 1) * src_tile_size],
            )

        if engine is not None:
            nisa.tensor_copy(
                dst_tensor[:, start_idx * dst_tile_size : end_idx * dst_tile_size],
                transposed_psum,
                engine=engine,
            )
        else:
            nisa.tensor_copy(
                dst_tensor[:, start_idx * dst_tile_size : end_idx * dst_tile_size],
                transposed_psum,
            )


def compute_rowsum_single_tile(
    o_tile: nl.ndarray,
    dy_transposed: nl.ndarray,
    dy_o_partial: List[nl.ndarray],
    i_d_head_tile: int,
    q_seq_tile_size: int,
    d_head_tile_size: int,
    num_tiles: int,
) -> None:
    """
    Compute one head tile's contribution to rowsum(dO ⊙ O).

    Transposes O, multiplies element-wise with transposed dY, reduces along
    the head dimension, and stores into dy_o_partial.

    Args:
        o_tile (nl.ndarray): O tensor tile, shape (d_head_tile_size, q_seq_tile_size * num_tiles).
        dy_transposed (nl.ndarray): Transposed dY, shape (q_seq_tile_size, d_head_tile_size * num_tiles).
        dy_o_partial (List[nl.ndarray]): Output list of tiles, each (q_seq_tile_size, d_head_n_tiles).
            Results written to dy_o_partial[i][:, i_d_head_tile].
        i_d_head_tile (int): Column index in dy_o_partial to write results.
        q_seq_tile_size (int): Query sequence tile size.
        d_head_tile_size (int): Head dimension tile size.
        num_tiles (int): Number of tiles to process.

    Returns:
        None
    """
    dy_o_mul = ndarray((num_tiles,), (q_seq_tile_size, d_head_tile_size), o_tile.dtype)

    for tile_idx in range(num_tiles):
        tmp_transpose_psum = nl.ndarray((q_seq_tile_size, d_head_tile_size), dtype=o_tile.dtype, buffer=nl.psum)
        nisa.nc_transpose(tmp_transpose_psum, o_tile[:, tile_idx * q_seq_tile_size : (tile_idx + 1) * q_seq_tile_size])
        nisa.tensor_tensor(
            dy_o_mul[tile_idx],
            dy_transposed[:, tile_idx * d_head_tile_size : (tile_idx + 1) * d_head_tile_size],
            tmp_transpose_psum,
            op=nl.multiply,
        )

    for tile_idx in range(num_tiles):
        nisa.tensor_reduce(dy_o_partial[tile_idx][:, i_d_head_tile], op=nl.add, data=dy_o_mul[tile_idx], axis=1)


def _load_tensor_pair(
    ref1: nl.ndarray,
    ref2: nl.ndarray,
    dtype: Any,
    d_head_n_tiles: int,
    d_head_tile_size: int,
    seq_tile_size: int,
    seqlen: int,
    offset: int,
    scale_first: Optional[float] = None,
) -> Tuple[List[nl.ndarray], List[nl.ndarray]]:
    """
    Load two tensors with the same access pattern from HBM.

    Args:
        ref1 (nl.ndarray): HBM reference for first tensor.
        ref2 (nl.ndarray): HBM reference for second tensor.
        dtype (Any): Data type for tiles.
        d_head_n_tiles (int): Number of tiles along head dimension.
        d_head_tile_size (int): Size of each head tile.
        seq_tile_size (int): Size of sequence tile to load.
        seqlen (int): Total sequence length (for stride calculation).
        offset (int): Base offset into HBM tensors.
        scale_first (Optional[float]): If provided, multiply first tensor by this value after loading.

    Returns:
        Tuple[List[nl.ndarray], List[nl.ndarray]]: Tuple of (tiles1, tiles2), each a list of length d_head_n_tiles
            containing tiles of shape (d_head_tile_size, seq_tile_size).
    """
    tiles1 = ndarray((d_head_n_tiles,), (d_head_tile_size, seq_tile_size), dtype)
    tiles2 = ndarray((d_head_n_tiles,), (d_head_tile_size, seq_tile_size), dtype)

    for d_head_tile_idx in range(d_head_n_tiles):
        nisa.dma_copy(
            dst=tiles1[d_head_tile_idx],
            src=ref1.ap(
                pattern=[[seqlen, d_head_tile_size], [1, seq_tile_size]],
                offset=offset + d_head_tile_idx * d_head_tile_size * seqlen,
            ),
        )
        if scale_first is not None:
            nisa.tensor_scalar(tiles1[d_head_tile_idx], tiles1[d_head_tile_idx], nl.multiply, scale_first)

        nisa.dma_copy(
            dst=tiles2[d_head_tile_idx],
            src=ref2.ap(
                pattern=[[seqlen, d_head_tile_size], [1, seq_tile_size]],
                offset=offset + d_head_tile_idx * d_head_tile_size * seqlen,
            ),
        )

    return tiles1, tiles2


def load_kv(
    k_ref_hbm_tile: nl.ndarray,
    v_ref_hbm_tile: nl.ndarray,
    dtype: Any,
    d_head_n_tiles: int,
    d_head_tile_size: int,
    k_seq_tile_size: int,
    seqlen_k: int,
    offset: int,
) -> Tuple[List[nl.ndarray], List[nl.ndarray]]:
    """
    Load K and V tiles from HBM.

    Args:
        k_ref_hbm_tile (nl.ndarray): HBM reference for K tensor.
        v_ref_hbm_tile (nl.ndarray): HBM reference for V tensor.
        dtype (Any): Data type for tiles.
        d_head_n_tiles (int): Number of tiles along head dimension.
        d_head_tile_size (int): Size of each head tile.
        k_seq_tile_size (int): Key sequence tile size.
        seqlen_k (int): Total key sequence length.
        offset (int): HBM byte offset.

    Returns:
        Tuple[List[nl.ndarray], List[nl.ndarray]]: Tuple of (k_local, v_local), each a list of length d_head_n_tiles
            containing tiles of shape (d_head_tile_size, k_seq_tile_size).
    """
    return _load_tensor_pair(
        ref1=k_ref_hbm_tile,
        ref2=v_ref_hbm_tile,
        dtype=dtype,
        d_head_n_tiles=d_head_n_tiles,
        d_head_tile_size=d_head_tile_size,
        seq_tile_size=k_seq_tile_size,
        seqlen=seqlen_k,
        offset=offset,
    )


def load_q_dy(
    q_ref_hbm_tile: nl.ndarray,
    dy_ref_hbm_tile: nl.ndarray,
    dtype: Any,
    d_head_n_tiles: int,
    d_head_tile_size: int,
    q_seq_tile_size: int,
    seqlen_q: int,
    offset: int,
    softmax_scale: float,
) -> Tuple[List[nl.ndarray], List[nl.ndarray]]:
    """
    Load Q and dY tiles from HBM, applying softmax scale to Q.

    Args:
        q_ref_hbm_tile (nl.ndarray): HBM reference for Q tensor.
        dy_ref_hbm_tile (nl.ndarray): HBM reference for dY tensor.
        dtype (Any): Data type for tiles.
        d_head_n_tiles (int): Number of tiles along head dimension.
        d_head_tile_size (int): Size of each head tile.
        q_seq_tile_size (int): Query sequence tile size.
        seqlen_q (int): Total query sequence length.
        offset (int): HBM byte offset.
        softmax_scale (float): Scale factor to apply to Q (typically 1/sqrt(d_head)).

    Returns:
        Tuple[List[nl.ndarray], List[nl.ndarray]]: Tuple of (q_local, dy_local), each a list of length d_head_n_tiles
            containing tiles of shape (d_head_tile_size, q_seq_tile_size).
            Q tiles are scaled in-place by softmax_scale.
    """
    return _load_tensor_pair(
        ref1=q_ref_hbm_tile,
        ref2=dy_ref_hbm_tile,
        dtype=dtype,
        d_head_n_tiles=d_head_n_tiles,
        d_head_tile_size=d_head_tile_size,
        seq_tile_size=q_seq_tile_size,
        seqlen=seqlen_q,
        offset=offset,
        scale_first=softmax_scale,
    )


def get_required_tiles_mask(
    q_tile_group_size: int,
    use_causal_mask: bool,
    i_q_seq_tile: int,
    q_seq_tile_size: int,
    k_seq_start: int,
    i_k_seq_tile: int,
    k_seq_tile_size: int,
    sliding_window: int,
) -> Tuple[List[bool], bool]:
    """
    Determine which tiles require computation based on causal and sliding window masks.

    Args:
        q_tile_group_size (int): Number of query tiles in the group.
        use_causal_mask (bool): Whether to apply causal masking.
        i_q_seq_tile (int): Current query sequence tile index.
        q_seq_tile_size (int): Size of each query tile.
        k_seq_start (int): Starting position of key sequence.
        i_k_seq_tile (int): Current key sequence tile index.
        k_seq_tile_size (int): Size of each key tile.
        sliding_window (int): Sliding window size.

    Returns:
        Tuple[List[bool], bool]: Tuple of (tile_required, any_tile_required):
            - tile_required: List of booleans indicating if each tile needs computation.
            - any_tile_required: True if at least one tile requires computation.
    """
    tile_required = []
    any_tile_required = False

    for i_q_tile_group_size in range(q_tile_group_size):
        # Tile-level early exit: Skip tiles where no query token can attend to any key token.
        if use_causal_mask:
            # Causal: max query position >= min key position
            q_tile_max_pos = (i_q_seq_tile + i_q_tile_group_size + 1) * q_seq_tile_size - 1
            k_tile_min_pos = k_seq_start + i_k_seq_tile * k_seq_tile_size
            _tile_required = q_tile_max_pos >= k_tile_min_pos

            if sliding_window > 0:
                # Sliding window: max key position >= earliest position any query can attend to
                q_tile_min_pos = (i_q_seq_tile + i_q_tile_group_size) * q_seq_tile_size
                k_tile_max_pos = k_seq_start + (i_k_seq_tile + 1) * k_seq_tile_size - 1
                earliest_attendable_pos = q_tile_min_pos - sliding_window + 1
                _tile_required = _tile_required and (k_tile_max_pos >= earliest_attendable_pos)

            tile_required.append(_tile_required)
            any_tile_required = any_tile_required or _tile_required
        else:
            tile_required.append(True)
            any_tile_required = True

    return tile_required, any_tile_required


def recompute_qk_softmax(
    cfg: AttentionBwdConfig,
    q_local: List[nl.ndarray],
    k_local: List[nl.ndarray],
    softmax_exp_bias: nl.ndarray,
    softmax_y: List[nl.ndarray],
    use_causal_mask: bool,
    sliding_window: int,
    tile_required: List[bool],
    q_tile_group_size: int,
    local_i_q_seq_tile: int,
    local_i_k_seq_tile: int,
    global_k_seq_offset: int = 0,
) -> None:
    """
    Compute Q @ K^T and apply softmax (recomputation for backward pass).

    Implements steps 2.1-2.4 of the backward algorithm:
        2.1: Compute Q @ K^T
        2.2: Scale is already applied to Q
        2.3: Apply causal mask
        2.4: Apply softmax using exp(score - lse)

    Args:
        cfg (AttentionBwdConfig): Attention backward configuration.
        q_local (List[nl.ndarray]): List of Q tiles, length d_head_n_tiles.
        k_local (List[nl.ndarray]): List of K tiles, length d_head_n_tiles.
        softmax_exp_bias (nl.ndarray): Negative LSE values for stable softmax,
            shape (q_seq_tile_size, q_seq_n_tiles).
        softmax_y (List[nl.ndarray]): Output list for softmax results, length q_tile_group_size,
            each (q_seq_tile_size, k_seq_tile_size).
        use_causal_mask (bool): Whether to apply causal masking.
        sliding_window (int): Sliding window size.
        tile_required (List[bool]): Boolean mask indicating which tiles to compute.
        q_tile_group_size (int): Number of Q tiles being processed.
        local_i_q_seq_tile (int): Current Q sequence tile index.
        local_i_k_seq_tile (int): Current K sequence tile index.
        global_k_seq_offset (int): Global offset for K sequence (for tiled impl).

    Returns:
        None
    """
    q_seq_tile_size = cfg.q_seq_tile_size
    k_seq_tile_size = cfg.k_seq_tile_size
    d_head_n_tiles = cfg.d_head_n_tiles

    qk_res_buf = ndarray((q_tile_group_size,), (q_seq_tile_size, k_seq_tile_size), nl.float32)
    for i_q_tile_group_size in range(q_tile_group_size):
        if not tile_required[i_q_tile_group_size]:
            continue

        qk_psum = nl.ndarray((q_seq_tile_size, k_seq_tile_size), buffer=nl.psum, dtype=nl.float32)

        for i_d_head_tile in range(d_head_n_tiles):
            nisa.nc_matmul(
                stationary=q_local[i_d_head_tile][
                    :, i_q_tile_group_size * q_seq_tile_size : (i_q_tile_group_size + 1) * q_seq_tile_size
                ],
                moving=k_local[i_d_head_tile],
                dst=qk_psum,
            )
        nisa.tensor_copy(qk_res_buf[i_q_tile_group_size], qk_psum)

    for i_q_tile_group_size in range(q_tile_group_size):
        if not tile_required[i_q_tile_group_size]:
            continue

        if use_causal_mask:
            nisa.affine_select(
                qk_res_buf[i_q_tile_group_size],
                pattern=[[-1, k_seq_tile_size]],
                offset=(local_i_q_seq_tile + i_q_tile_group_size) * q_seq_tile_size
                - (global_k_seq_offset + local_i_k_seq_tile * k_seq_tile_size),
                channel_multiplier=1,
                cmp_op=nl.greater_equal,
                on_true_tile=qk_res_buf[i_q_tile_group_size],
                on_false_value=_FLOAT32_MIN,
            )

            if sliding_window > 0:
                nisa.affine_select(
                    qk_res_buf[i_q_tile_group_size],
                    pattern=[[1, k_seq_tile_size]],
                    offset=(global_k_seq_offset + local_i_k_seq_tile * k_seq_tile_size)
                    - ((local_i_q_seq_tile + i_q_tile_group_size) * q_seq_tile_size - sliding_window + 1),
                    channel_multiplier=-1,
                    cmp_op=nl.greater_equal,
                    on_true_tile=qk_res_buf[i_q_tile_group_size],
                    on_false_value=_FLOAT32_MIN,
                )

        nisa.activation(
            dst=softmax_y[i_q_tile_group_size],
            op=nl.exp,
            data=qk_res_buf[i_q_tile_group_size],
            bias=softmax_exp_bias[:, (local_i_q_seq_tile + i_q_tile_group_size)],
            scale=1.0,
        )


def compute_softmax_backward_dx(
    cfg: AttentionBwdConfig,
    dy_local: List[nl.ndarray],
    v_local: List[nl.ndarray],
    softmax_y: List[nl.ndarray],
    dy_o_sum: nl.ndarray,
    tile_required: List[bool],
    q_tile_group_size: int,
    softmax_dx_local: List[nl.ndarray],
) -> None:
    """
    Compute gradients through softmax for flash attention backward.

    Combines:
        - Step 3.2: dL/d(softmax) = dY @ V^T
        - Step 4: dL/dx = softmax * (dL/d(softmax) - D), where D = rowsum(dO ⊙ O)

    Args:
        cfg (AttentionBwdConfig): Attention backward configuration.
        dy_local (List[nl.ndarray]): List of dY tiles, length d_head_n_tiles,
            each (d_head_tile_size, q_seq_tile_size * q_tile_group_size).
        v_local (List[nl.ndarray]): List of V tiles, length d_head_n_tiles,
            each (d_head_tile_size, k_seq_tile_size).
        softmax_y (List[nl.ndarray]): Recomputed attention weights, length q_tile_group_size,
            each (q_seq_tile_size, k_seq_tile_size).
        dy_o_sum (nl.ndarray): D = rowsum(dO ⊙ O), shape (q_seq_tile_size, q_tile_group_size).
        tile_required (List[bool]): Boolean mask indicating which tiles to compute.
        q_tile_group_size (int): Number of Q tiles being processed.
        softmax_dx_local (List[nl.ndarray]): Output list for gradients, length q_tile_group_size,
            each (q_seq_tile_size, k_seq_tile_size).

    Returns:
        None
    """
    q_seq_tile_size = cfg.q_seq_tile_size
    k_seq_tile_size = cfg.k_seq_tile_size
    d_head_n_tiles = cfg.d_head_n_tiles

    for i_q_tile_group_size in range(q_tile_group_size):
        if not tile_required[i_q_tile_group_size]:
            continue

        # Step 3.2: Calculate the backward gradients dL/dsoftmax, where y=softmax@V.
        # Computes value projection gradient with matmul(stationary=dy, moving=v).
        softmax_dy_psum = nl.ndarray((q_seq_tile_size, k_seq_tile_size), dtype=nl.float32, buffer=nl.psum)
        for i_d_head_tile in range(d_head_n_tiles):
            nisa.nc_matmul(
                stationary=dy_local[i_d_head_tile][
                    :, i_q_tile_group_size * q_seq_tile_size : (i_q_tile_group_size + 1) * q_seq_tile_size
                ],
                moving=v_local[i_d_head_tile],
                dst=softmax_dy_psum,
            )

        # Step 4: Calculate the softmax backward gradients dL/dx, where y=softmax(x).
        # Computes: dL/dx = y * (dL/dy - rowsum(dO_O)), where y = softmax(x)
        nisa.scalar_tensor_tensor(
            dst=softmax_dx_local[i_q_tile_group_size],
            data=softmax_dy_psum,
            op0=nl.subtract,
            operand0=dy_o_sum[:, i_q_tile_group_size],
            op1=nl.multiply,
            operand1=softmax_y[i_q_tile_group_size],
        )


def flash_attn_bwd(
    out_dq_ref: nl.ndarray,
    out_dk_ref: nl.ndarray,
    out_dv_ref: nl.ndarray,
    out_dsinks_ref: Optional[nl.ndarray],
    q_ref: nl.ndarray,
    k_ref: nl.ndarray,
    v_ref: nl.ndarray,
    o_ref: nl.ndarray,
    dy_ref: nl.ndarray,
    lse_ref: nl.ndarray,
    sinks_ref: Optional[nl.ndarray],
    use_causal_mask: bool,
    mixed_precision: bool,
    softmax_scale: float,
    sliding_window: int,
) -> None:
    """
    Flash attention backward pass.

    Processes KV sequence in sections of k_seq_section_len (max 8K) to
    manage memory for longer sequence length. Each section computes partial
    query gradients that are accumulated across sections. Also, outer loop on
    query tensor helps manage memory for large Q/KV head ratio.

    Args:
        out_dq_ref: Output dQ tensor in HBM.
        out_dk_ref: Output dK tensor in HBM.
        out_dv_ref: Output dV tensor in HBM.
        out_dsinks_ref: Output attention sinks grad tensor in HBM.
        q_ref: Input Q tensor.
        k_ref: Input K tensor.
        v_ref: Input V tensor.
        o_ref: Forward output O tensor.
        dy_ref: Gradient dO from upstream.
        lse_ref: Log-sum-exp from forward pass.
        sinks_ref: Attention sink tokens in HBM.
        use_causal_mask: Whether to apply causal masking.
        mixed_precision: Whether to use float32 for accumulations.
        softmax_scale: Attention score scaling factor.
        sliding_window: Sliding window size.

    Note:
        Parallelization is across batch * nheads_kv dimension.
        dQ is written incrementally and reloaded for each K section.
    """
    cfg = setup_config(q_ref, k_ref, sinks_ref, mixed_precision)

    # Common variables
    bs = cfg.bs
    seqlen_k = cfg.seqlen_k
    seqlen_q = cfg.seqlen_q
    nheads_kv = cfg.nheads_kv
    nheads_per_kv_head = cfg.nheads_per_kv_head

    d_head_n_tiles = cfg.d_head_n_tiles
    d_head_tile_size = cfg.d_head_tile_size
    k_seq_tile_size = cfg.k_seq_tile_size
    q_seq_tile_size = cfg.q_seq_tile_size
    q_seq_n_tiles = cfg.q_seq_n_tiles

    kernel_dtype = cfg.kernel_dtype
    mixed_dtype = cfg.mixed_dtype

    # Calculate start and end indices for sharded input
    if nl.program_ndim() == 0:
        num_shards, shard_id = 1, 0
    else:
        num_shards, shard_id = nl.num_programs(0), nl.program_id(0)

    shard_size = div_ceil(bs * nheads_kv, num_shards)
    start_idx = shard_id * shard_size
    end_idx = min(start_idx + shard_size, bs * nheads_kv)

    for sample_idx in range(start_idx, end_idx):
        batch_id = sample_idx // nheads_kv
        head_id = sample_idx % nheads_kv

        dy_o_sum = ndarray((nheads_per_kv_head,), (q_seq_tile_size, q_seq_n_tiles), dtype=mixed_dtype, value=0.0)

        # Step 2.4 Prefetch exp bias for softmax
        softmax_exp_bias = ndarray((nheads_per_kv_head,), (q_seq_tile_size, q_seq_n_tiles), mixed_dtype)
        for i_q_head in range(nheads_per_kv_head):
            offset_lse = batch_id * cfg.offset_lse_bs + (head_id * nheads_per_kv_head + i_q_head) * cfg.offset_lse_head
            nisa.dma_copy(
                dst=softmax_exp_bias[i_q_head],
                src=lse_ref.ap(
                    pattern=[[q_seq_n_tiles, q_seq_tile_size], [1, q_seq_n_tiles]],
                    offset=offset_lse,
                ),
            )
            nisa.tensor_scalar(softmax_exp_bias[i_q_head], softmax_exp_bias[i_q_head], nl.multiply, -1.0)

        for k_seq_start in range(0, seqlen_k, cfg.k_seq_section_len):
            cur_k_section_len = min(cfg.k_seq_section_len, seqlen_k - k_seq_start)
            cur_k_seq_n_tiles = cur_k_section_len // k_seq_tile_size

            dk_local_reduced = ndarray((d_head_n_tiles,), (d_head_tile_size, cur_k_section_len), mixed_dtype, value=0.0)
            dv_local_reduced = ndarray((d_head_n_tiles,), (d_head_tile_size, cur_k_section_len), mixed_dtype, value=0.0)

            k_loaded, v_loaded = load_kv(
                k_ref_hbm_tile=k_ref,
                v_ref_hbm_tile=v_ref,
                dtype=cfg.kernel_dtype,
                d_head_n_tiles=d_head_n_tiles,
                d_head_tile_size=d_head_tile_size,
                k_seq_tile_size=cur_k_section_len,
                seqlen_k=seqlen_k,
                offset=batch_id * cfg.offset_k_bs + head_id * cfg.offset_k_head + k_seq_start,
            )

            for i_q_head in range(nheads_per_kv_head):
                q_head_offset = head_id * nheads_per_kv_head

                # Setup dq tensor (initialize with zero for the first iteration, reload otherwise)
                for i_q_seq_tile in range(0, q_seq_n_tiles, cfg.q_tile_group_size):
                    cur_q_tile_group_size = min(cfg.q_tile_group_size, q_seq_n_tiles - i_q_seq_tile)
                    offset_q = batch_id * cfg.offset_q_bs + (i_q_head + q_head_offset) * cfg.offset_q_head

                    dq_local = ndarray(
                        (d_head_n_tiles,),
                        (d_head_tile_size, q_seq_tile_size * cur_q_tile_group_size),
                        dtype=mixed_dtype,
                        value=0.0 if k_seq_start == 0 else None,
                    )
                    if k_seq_start > 0:
                        for i_d_head_tile in range(d_head_n_tiles):
                            nisa.dma_copy(
                                dq_local[i_d_head_tile],
                                out_dq_ref.ap(
                                    pattern=[
                                        [seqlen_q, d_head_tile_size],
                                        [1, q_seq_tile_size * cur_q_tile_group_size],
                                    ],
                                    offset=offset_q
                                    + i_d_head_tile * d_head_tile_size * seqlen_q
                                    + i_q_seq_tile * q_seq_tile_size,
                                ),
                            )

                    # Load and transpose q, dy
                    q_local, dy_local = load_q_dy(
                        q_ref_hbm_tile=q_ref,
                        dy_ref_hbm_tile=dy_ref,
                        dtype=cfg.kernel_dtype,
                        d_head_n_tiles=d_head_n_tiles,
                        d_head_tile_size=d_head_tile_size,
                        q_seq_tile_size=q_seq_tile_size * cur_q_tile_group_size,
                        seqlen_q=seqlen_q,
                        offset=offset_q + i_q_seq_tile * q_seq_tile_size,
                        softmax_scale=softmax_scale,
                    )

                    trans_q_local = ndarray(
                        (d_head_n_tiles,), (q_seq_tile_size, d_head_tile_size * cur_q_tile_group_size), kernel_dtype
                    )
                    dy_local_transposed = ndarray(
                        (d_head_n_tiles,), (q_seq_tile_size, d_head_tile_size * cur_q_tile_group_size), kernel_dtype
                    )
                    for i_d_head_tile in range(d_head_n_tiles):
                        transpose_tiles(q_local[i_d_head_tile], trans_q_local[i_d_head_tile], q_seq_tile_size)
                        transpose_tiles(dy_local[i_d_head_tile], dy_local_transposed[i_d_head_tile], q_seq_tile_size)

                    # Compute dy_o_sum for current q tiles
                    if k_seq_start == 0:
                        o_local = ndarray(
                            (d_head_n_tiles,), (d_head_tile_size, q_seq_tile_size * cur_q_tile_group_size), kernel_dtype
                        )  # o
                        for i_d_head_tile in range(d_head_n_tiles):
                            nisa.dma_copy(
                                dst=o_local[i_d_head_tile],
                                src=o_ref.ap(
                                    pattern=[
                                        [seqlen_q, d_head_tile_size],
                                        [1, q_seq_tile_size * cur_q_tile_group_size],
                                    ],
                                    offset=offset_q
                                    + i_d_head_tile * d_head_tile_size * seqlen_q
                                    + i_q_seq_tile * q_seq_tile_size,
                                ),
                            )

                        dy_o_partial = ndarray((cur_q_tile_group_size,), (q_seq_tile_size, d_head_n_tiles), mixed_dtype)
                        for i_d_head_tile in range(d_head_n_tiles):
                            compute_rowsum_single_tile(
                                o_local[i_d_head_tile],
                                dy_local_transposed[i_d_head_tile],
                                dy_o_partial,
                                i_d_head_tile=i_d_head_tile,
                                q_seq_tile_size=q_seq_tile_size,
                                d_head_tile_size=d_head_tile_size,
                                num_tiles=cur_q_tile_group_size,
                            )

                        # reduce along all head tile results
                        for i_q_tile_group_size in range(cur_q_tile_group_size):
                            nisa.tensor_reduce(
                                dy_o_sum[i_q_head][:, i_q_seq_tile + i_q_tile_group_size],
                                op=nl.add,
                                data=dy_o_partial[i_q_tile_group_size],
                                axis=1,
                            )

                    # Compute dq, dk, dv tiles
                    offset_k = batch_id * cfg.offset_k_bs + head_id * cfg.offset_k_head
                    for i_k_seq_tile in range(cur_k_seq_n_tiles):
                        tile_required, any_tile_required = get_required_tiles_mask(
                            cur_q_tile_group_size,
                            use_causal_mask,
                            i_q_seq_tile,
                            q_seq_tile_size,
                            k_seq_start,
                            i_k_seq_tile,
                            k_seq_tile_size,
                            sliding_window,
                        )

                        if any_tile_required:
                            v_local, k_local = [], []
                            for i_d_head_tile in range(d_head_n_tiles):
                                k_local.append(
                                    k_loaded[i_d_head_tile][
                                        :, i_k_seq_tile * k_seq_tile_size : (i_k_seq_tile + 1) * k_seq_tile_size
                                    ]
                                )
                                v_local.append(
                                    v_loaded[i_d_head_tile][
                                        :, i_k_seq_tile * k_seq_tile_size : (i_k_seq_tile + 1) * k_seq_tile_size
                                    ]
                                )

                            _flash_attn_bwd_core(
                                cfg,
                                q_local=q_local,
                                k_local=k_local,
                                v_local=v_local,
                                dy_local=dy_local,
                                dk_local_reduced=dk_local_reduced,
                                dv_local_reduced=dv_local_reduced,
                                dq_local=dq_local,
                                softmax_exp_bias=softmax_exp_bias[i_q_head],
                                dy_o_sum=dy_o_sum[i_q_head][:, i_q_seq_tile : i_q_seq_tile + cur_q_tile_group_size],
                                trans_q_local=trans_q_local,
                                trans_dy=dy_local_transposed,
                                local_i_q_seq_tile=i_q_seq_tile,
                                local_i_k_seq_tile=i_k_seq_tile,
                                use_causal_mask=use_causal_mask,
                                softmax_scale=softmax_scale,
                                sliding_window=sliding_window,
                                tile_required=tile_required,
                                q_tile_group_size=cur_q_tile_group_size,
                                global_k_seq_offset=k_seq_start,
                            )

                    # Write dq
                    for i_d_head_tile in range(d_head_n_tiles):
                        nisa.dma_copy(
                            out_dq_ref.ap(
                                pattern=[[seqlen_q, d_head_tile_size], [1, q_seq_tile_size * cur_q_tile_group_size]],
                                offset=offset_q
                                + i_d_head_tile * d_head_tile_size * seqlen_q
                                + i_q_seq_tile * q_seq_tile_size,
                            ),
                            dq_local[i_d_head_tile],
                        )

            # Write dk, dv
            for i_d_head_tile in range(d_head_n_tiles):
                offset_k = batch_id * cfg.offset_k_bs + head_id * cfg.offset_k_head
                nisa.dma_copy(
                    out_dk_ref.ap(
                        pattern=[[seqlen_k, d_head_tile_size], [1, cur_k_section_len]],
                        offset=offset_k + i_d_head_tile * d_head_tile_size * seqlen_k + k_seq_start,
                    ),
                    dk_local_reduced[i_d_head_tile],
                )
                nisa.dma_copy(
                    out_dv_ref.ap(
                        pattern=[[seqlen_k, d_head_tile_size], [1, cur_k_section_len]],
                        offset=offset_k + i_d_head_tile * d_head_tile_size * seqlen_k + k_seq_start,
                    ),
                    dv_local_reduced[i_d_head_tile],
                )

        # Compute attention sinks grad
        if cfg.num_sinks > 0:
            dsinks_local = nl.ndarray((q_seq_tile_size, nheads_per_kv_head, cfg.num_sinks), dtype=mixed_dtype)
            nisa.memset(dsinks_local, value=0.0)

            # Load sinks
            sink_sigma = nl.ndarray((q_seq_tile_size, nheads_per_kv_head, cfg.num_sinks), dtype=mixed_dtype)
            nisa.dma_copy(
                sink_sigma,
                sinks_ref.ap(
                    pattern=[
                        [0, q_seq_tile_size],
                        [cfg.num_sinks, nheads_per_kv_head],
                        [1, cfg.num_sinks],
                    ],
                    offset=batch_id * cfg.offset_sinks_bs + head_id * nheads_per_kv_head * cfg.offset_sinks_head,
                ),
            )

            # Compute dsinks
            for i_q_head in range(nheads_per_kv_head):
                for i_q_seq_tile in range(q_seq_n_tiles):
                    p_sink = nl.ndarray((q_seq_tile_size, cfg.num_sinks), dtype=cfg.mixed_dtype)
                    nisa.activation(
                        dst=p_sink,
                        op=nl.exp,
                        data=sink_sigma[:, i_q_head],
                        bias=softmax_exp_bias[i_q_head][:, i_q_seq_tile],
                        scale=1.0,
                    )

                    nisa.scalar_tensor_tensor(
                        dsinks_local[:, i_q_head],
                        data=p_sink,
                        op0=nl.multiply,
                        operand0=dy_o_sum[i_q_head][:, i_q_seq_tile],
                        op1=nl.subtract,
                        operand1=dsinks_local[:, i_q_head],
                        reverse1=True,
                    )

            # Write dsinks
            dsinks_reduced = nl.ndarray((1, nheads_per_kv_head, cfg.num_sinks), dtype=mixed_dtype)
            nisa.tensor_partition_reduce(dsinks_reduced, nl.add, dsinks_local)
            nisa.dma_copy(
                out_dsinks_ref.ap(
                    pattern=[[0, 1], [cfg.num_sinks, nheads_per_kv_head], [1, cfg.num_sinks]],
                    offset=batch_id * cfg.offset_sinks_bs + head_id * nheads_per_kv_head * cfg.offset_sinks_head,
                ),
                dsinks_reduced,
            )


def _flash_attn_bwd_core(
    cfg: AttentionBwdConfig,
    q_local: List[nl.ndarray],
    k_local: List[nl.ndarray],
    v_local: List[nl.ndarray],
    dy_local: List[nl.ndarray],
    dk_local_reduced: List[nl.ndarray],
    dv_local_reduced: List[nl.ndarray],
    dq_local: List[nl.ndarray],
    softmax_exp_bias: nl.ndarray,
    dy_o_sum: nl.ndarray,
    trans_q_local: List[nl.ndarray],
    trans_dy: List[nl.ndarray],
    local_i_q_seq_tile: int,
    local_i_k_seq_tile: int,
    use_causal_mask: bool,
    softmax_scale: float,
    sliding_window: int,
    tile_required: List[bool],
    q_tile_group_size: int = 1,
    global_k_seq_offset: int = 0,
) -> None:
    """
    Core backward pass computation for flash attention.

    Args:
        cfg: Attention backward configuration.
        q_local: Q tiles (pre-scaled), length d_head_n_tiles.
        k_local: K tiles for current section, length d_head_n_tiles.
        v_local: V tiles for current section, length d_head_n_tiles.
        dy_local: Gradient dO tiles, length d_head_n_tiles.
        dk_local_reduced: Accumulator for dK (sbuf), covers full K section.
        dv_local_reduced: Accumulator for dV (sbuf), covers full K section.
        dq_local: Accumulator for dQ (sbuf), covers current Q tile group.
        softmax_exp_bias: Negative LSE for stable softmax.
        dy_o_sum: D = rowsum(dO ⊙ O) for softmax backward.
        trans_q_local: Pre-transposed Q tiles (reused across K tiles).
        trans_dy: Pre-transposed dY tiles (reused across K tiles).
        local_i_q_seq_tile: Current Q sequence tile index.
        local_i_k_seq_tile: Current K sequence tile index within section.
        use_causal_mask: Whether to apply causal masking.
        softmax_scale: Attention score scaling factor.
        sliding_window: Sliding window size.
        tile_required: Boolean mask indicating which tiles to compute.
        q_tile_group_size: Number of Q tiles being processed.
        global_k_seq_offset: Global offset for K sequence position.
    """
    q_seq_tile_size = cfg.q_seq_tile_size
    k_seq_tile_size = cfg.k_seq_tile_size
    k_seq_tile_size_backward = cfg.k_seq_tile_size_backward
    k_seq_fwd_bwd_tile_multiplier = cfg.k_seq_fwd_bwd_multiplier
    d_head_n_tiles = cfg.d_head_n_tiles
    d_head_tile_size = cfg.d_head_tile_size
    kernel_dtype = cfg.kernel_dtype

    # Step 2. Recompute (softmax(Q^T@K))
    softmax_y = ndarray((q_tile_group_size,), (q_seq_tile_size, k_seq_tile_size), kernel_dtype)
    recompute_qk_softmax(
        cfg,
        q_local,
        k_local,
        softmax_exp_bias,
        softmax_y,
        use_causal_mask,
        sliding_window,
        tile_required,
        q_tile_group_size,
        local_i_q_seq_tile,
        local_i_k_seq_tile,
        global_k_seq_offset=global_k_seq_offset,
    )

    # Step 4: Calculate the softmax backward gradients dL/dx, where y=softmax(x).
    # Computes: dL/dx = y * (dL/dy - rowsum(dO_O)), where y = softmax(x)
    softmax_dx_local = ndarray((q_tile_group_size,), (q_seq_tile_size, k_seq_tile_size), kernel_dtype)
    compute_softmax_backward_dx(
        cfg=cfg,
        dy_local=dy_local,
        v_local=v_local,
        softmax_y=softmax_y,
        dy_o_sum=dy_o_sum,
        tile_required=tile_required,
        q_tile_group_size=q_tile_group_size,
        softmax_dx_local=softmax_dx_local,
    )

    # Step 5.2 Calculate dQ
    transposed_k_local = ndarray(
        (d_head_n_tiles,),
        (k_seq_tile_size_backward, d_head_tile_size * k_seq_fwd_bwd_tile_multiplier),
        k_local[0].dtype,
    )
    for i_d_head_tile in range(d_head_n_tiles):
        transpose_tiles(
            k_local[i_d_head_tile],
            transposed_k_local[i_d_head_tile],
            k_seq_tile_size_backward,
            engine=nisa.scalar_engine,
        )

    transposed_softmax_dx_local = ndarray(
        (k_seq_fwd_bwd_tile_multiplier,),
        (k_seq_tile_size_backward, q_seq_tile_size * q_tile_group_size),
        softmax_dx_local[0].dtype,
    )
    for i_k_seq_tile_backward in range(k_seq_fwd_bwd_tile_multiplier):
        transposed_softmax_dx_local_psum = nl.ndarray(
            (k_seq_tile_size_backward, q_seq_tile_size * q_tile_group_size),
            dtype=softmax_dx_local[0].dtype,
            buffer=nl.psum,
        )
        for i_q_tile_group_size in range(q_tile_group_size):
            if not tile_required[i_q_tile_group_size]:
                continue

            nisa.nc_transpose(
                transposed_softmax_dx_local_psum[
                    :, i_q_tile_group_size * q_seq_tile_size : (i_q_tile_group_size + 1) * q_seq_tile_size
                ],
                softmax_dx_local[i_q_tile_group_size][
                    :,
                    i_k_seq_tile_backward * k_seq_tile_size_backward : (i_k_seq_tile_backward + 1)
                    * k_seq_tile_size_backward,
                ],
            )

        nisa.tensor_copy(transposed_softmax_dx_local[i_k_seq_tile_backward], transposed_softmax_dx_local_psum)

    for i_d_head_tile in range(d_head_n_tiles):
        dq_psum = nl.ndarray((d_head_tile_size, q_seq_tile_size * q_tile_group_size), dtype=nl.float32, buffer=nl.psum)
        nisa.memset(dq_psum, value=0.0)
        for i_k_seq_tile_backward in range(k_seq_fwd_bwd_tile_multiplier):
            for i_q_tile_group_size in range(q_tile_group_size):
                if not tile_required[i_q_tile_group_size]:
                    continue

                nisa.nc_matmul(
                    stationary=transposed_k_local[i_d_head_tile][
                        :, i_k_seq_tile_backward * d_head_tile_size : (i_k_seq_tile_backward + 1) * d_head_tile_size
                    ],
                    moving=transposed_softmax_dx_local[i_k_seq_tile_backward][
                        :, i_q_tile_group_size * q_seq_tile_size : (i_q_tile_group_size + 1) * q_seq_tile_size
                    ],
                    dst=dq_psum[:, i_q_tile_group_size * q_seq_tile_size : (i_q_tile_group_size + 1) * q_seq_tile_size],
                )

        nisa.scalar_tensor_tensor(
            dst=dq_local[i_d_head_tile],
            data=dq_psum,
            op0=nl.multiply,
            operand0=softmax_scale,
            op1=nl.add,
            operand1=dq_local[i_d_head_tile],
        )

    # Step 3.1: Calculate dV, with matmul(stationary=dy, moving=softmax) where y=softmax@V
    for i_d_head_tile in range(d_head_n_tiles):
        dv_psum = nl.ndarray((d_head_tile_size, k_seq_tile_size), dtype=nl.float32, buffer=nl.psum)
        for i_q_tile_group_size in range(q_tile_group_size):
            if not tile_required[i_q_tile_group_size]:
                continue

            nisa.nc_matmul(
                stationary=trans_dy[i_d_head_tile][
                    :, i_q_tile_group_size * d_head_tile_size : (i_q_tile_group_size + 1) * d_head_tile_size
                ],
                moving=softmax_y[i_q_tile_group_size][:, :],
                dst=dv_psum,
            )

        nisa.tensor_tensor(
            dv_local_reduced[i_d_head_tile][
                :, local_i_k_seq_tile * k_seq_tile_size : (local_i_k_seq_tile + 1) * k_seq_tile_size
            ],
            data1=dv_local_reduced[i_d_head_tile][
                :, local_i_k_seq_tile * k_seq_tile_size : (local_i_k_seq_tile + 1) * k_seq_tile_size
            ],
            data2=dv_psum,
            op=nl.add,
        )

    # Step 5.1 Calculate dK, with matmul(stationary=Q, moving=softmax_dx)
    for i_d_head_tile in range(d_head_n_tiles):
        dk_psum = nl.ndarray((d_head_tile_size, k_seq_tile_size), dtype=nl.float32, buffer=nl.psum)
        for i_q_tile_group_size in range(q_tile_group_size):
            if not tile_required[i_q_tile_group_size]:
                continue

            nisa.nc_matmul(
                stationary=trans_q_local[i_d_head_tile][
                    :, i_q_tile_group_size * d_head_tile_size : (i_q_tile_group_size + 1) * d_head_tile_size
                ],
                moving=softmax_dx_local[i_q_tile_group_size],
                dst=dk_psum,
            )

        nisa.tensor_tensor(
            dk_local_reduced[i_d_head_tile][
                :, local_i_k_seq_tile * k_seq_tile_size : (local_i_k_seq_tile + 1) * k_seq_tile_size
            ],
            data1=dk_local_reduced[i_d_head_tile][
                :, local_i_k_seq_tile * k_seq_tile_size : (local_i_k_seq_tile + 1) * k_seq_tile_size
            ],
            data2=dk_psum,
            op=nl.add,
        )
